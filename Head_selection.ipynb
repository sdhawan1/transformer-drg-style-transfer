{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Head_selection.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jMjcFHyhi5vI","colab_type":"text"},"source":["# Head Selection\n","**_BERT_** is a **_Multi-layer_ _Multi-Head_** Transformer architecture. As discuss in many of the current reseachers, different Attention heads captures different lingustic patterns. For a better deletion of words using Attention mechanism we need to choose a head which **captures pattern useful for classification.**\n","\n","To do this we are using a Brute force mechanism to seach through all the possible heads. We are deleting TopK words attended by different heads from the sentence and measuring the new classification score. In case of sentiments, removing sentiments related words makes the sentence neutral. The heads are sorted by the amount to which it is able to make the sentences from dev set to Neutral."]},{"cell_type":"code","metadata":{"id":"RvS6rEFSkNsE","colab_type":"code","outputId":"7b059c52-6944-42a3-8944-25025e2e2fad","executionInfo":{"status":"ok","timestamp":1591164310309,"user_tz":420,"elapsed":18923,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","#drive.mount('/content/gdrive')\n","\n","drive.mount(\"/content/gdrive\", force_remount=False)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wIDdP_itk-va","colab_type":"code","outputId":"59ed44d3-9b7d-4804-95cf-480e9b6ac9c8","executionInfo":{"status":"ok","timestamp":1591164317582,"user_tz":420,"elapsed":3640,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["## specify hardware.\n","import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I7997Dg3ZI3W","colab_type":"code","outputId":"bd672f8d-f91c-4a5f-91fb-3983b21eff7b","executionInfo":{"status":"ok","timestamp":1591164320445,"user_tz":420,"elapsed":376,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["import sys\n","print(sys.path)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B-4n04ffi5vL","colab_type":"code","outputId":"d7f431c4-c1f7-4c69-8b81-66cf20c035b8","executionInfo":{"status":"ok","timestamp":1591164334872,"user_tz":420,"elapsed":10778,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import csv\n","import logging\n","import os\n","import random\n","import sys\n","import numpy as np\n","import torch\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n","                              TensorDataset)\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","\n","#load the forked \"pretrained_bert_model\" settings included in the drg repo (use this for bertviz stuff too...)\n","drg_repo_path = '/content/gdrive/My Drive/humor_style_transfer/transformer-drg-style-transfer'\n","if not drg_repo_path in sys.path:\n","  sys.path.append(drg_repo_path)\n","\n","from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n","from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n","#from pytorch_pretrained_bert.tokenization import BertTokenizer\n","from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear #(deprecated)\n","\n","from bertviz.bertviz import attention, visualization\n","from bertviz.bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer\n","\n","logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                    datefmt = '%m/%d/%Y %H:%M:%S',\n","                    level = logging.INFO)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CVFL9_Tyi5vV","colab_type":"code","outputId":"f09b4521-def1-4933-cece-b77ae16654d4","executionInfo":{"status":"ok","timestamp":1591164337998,"user_tz":420,"elapsed":421,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["logger = logging.getLogger(__name__)\n","## TO CUSTOMIZE: Path of BERT classifier model\n","bert_classifier_model_dir = \"/content/gdrive/My Drive/humor_style_transfer/transformer-drg-style-transfer/reddit_jokes_scripts/bert_classifier\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","logger.info(\"device: {}, n_gpu {}\".format(device, n_gpu))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["06/03/2020 06:05:37 - INFO - __main__ -   device: cuda, n_gpu 1\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"mc5qgTF7i5vh","colab_type":"code","outputId":"2cca8cc3-3bf4-4121-fab0-131f1ea0af81","executionInfo":{"status":"ok","timestamp":1591164364836,"user_tz":420,"elapsed":22457,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["## Model for performing Classification\n","model_cls = BertForSequenceClassification.from_pretrained(bert_classifier_model_dir, num_labels=2)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","model_cls.to(device)\n","model_cls.eval()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["06/03/2020 06:05:42 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /content/gdrive/My Drive/humor_style_transfer/transformer-drg-style-transfer/reddit_jokes_scripts/bert_classifier\n","06/03/2020 06:05:42 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","06/03/2020 06:06:04 - INFO - bertviz.bertviz.pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmp8i8t_3u0\n","100%|██████████| 231508/231508 [00:00<00:00, 1951737.80B/s]\n","06/03/2020 06:06:04 - INFO - bertviz.bertviz.pytorch_pretrained_bert.file_utils -   copying /tmp/tmp8i8t_3u0 to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","06/03/2020 06:06:04 - INFO - bertviz.bertviz.pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","06/03/2020 06:06:04 - INFO - bertviz.bertviz.pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmp8i8t_3u0\n","06/03/2020 06:06:04 - INFO - bertviz.bertviz.pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): BertLayerNorm()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"Gnx68J98i5vo","colab_type":"code","outputId":"caa203e6-7a56-4cec-e16d-98011e757486","executionInfo":{"status":"ok","timestamp":1591164384173,"user_tz":420,"elapsed":3315,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["## Model to get the attention weights of all the heads\n","model = BertModel.from_pretrained(bert_classifier_model_dir)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","model.to(device)\n","model.eval()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["06/03/2020 06:06:20 - INFO - bertviz.bertviz.pytorch_pretrained_bert.modeling -   loading archive file /content/gdrive/My Drive/humor_style_transfer/transformer-drg-style-transfer/reddit_jokes_scripts/bert_classifier\n","06/03/2020 06:06:20 - INFO - bertviz.bertviz.pytorch_pretrained_bert.modeling -   Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","06/03/2020 06:06:23 - INFO - bertviz.bertviz.pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): BertLayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"wjN1Nym9i5vt","colab_type":"code","colab":{}},"source":["max_seq_len=70 # Maximum sequence length \n","sm = torch.nn.Softmax(dim=-1) ## Softmax over the batch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LxpbhyRYi5v0","colab_type":"code","colab":{}},"source":["def run_multiple_examples(input_sentences, bs=32):\n","    \"\"\"\n","    This fucntion returns classification predictions for batch of sentences.\n","    input_sentences: list of strings\n","    bs : batch_size : int\n","    \"\"\"\n","    \n","    ## Prepare data for classification\n","    ids = []\n","    segment_ids = []\n","    input_masks = []\n","    pred_lt = []\n","    for sen in input_sentences:\n","        text_tokens = tokenizer.tokenize(sen)\n","        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n","        temp_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        input_mask = [1] * len(temp_ids)\n","        segment_id = [0] * len(temp_ids)\n","        padding = [0] * (max_seq_len - len(temp_ids))\n","\n","        temp_ids += padding\n","        input_mask += padding\n","        segment_id += padding\n","        \n","        ids.append(temp_ids)\n","        input_masks.append(input_mask)\n","        segment_ids.append(segment_id)\n","    \n","    ## Convert input lists to Torch Tensors\n","    ids = torch.tensor(ids)\n","    segment_ids = torch.tensor(segment_ids)\n","    input_masks = torch.tensor(input_masks)\n","    \n","    steps = len(ids) // bs\n","    \n","    for i in range(steps+1):\n","        if i == steps:\n","            temp_ids = ids[i * bs : len(ids)]\n","            temp_segment_ids = segment_ids[i * bs: len(ids)]\n","            temp_input_masks = input_masks[i * bs: len(ids)]\n","        else:\n","            temp_ids = ids[i * bs : i * bs + bs]\n","            temp_segment_ids = segment_ids[i * bs: i * bs + bs]\n","            temp_input_masks = input_masks[i * bs: i * bs + bs]\n","        \n","        temp_ids = temp_ids.to(device)\n","        temp_segment_ids = temp_segment_ids.to(device)\n","        temp_input_masks = temp_input_masks.to(device)\n","        \n","        with torch.no_grad():\n","            preds = sm(model_cls(temp_ids, temp_segment_ids, temp_input_masks))\n","        pred_lt.extend(preds.tolist())\n","    \n","    return pred_lt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wGJOvB90i5v9","colab_type":"code","colab":{}},"source":["def read_file(path,size):\n","    with open(path) as fp:\n","        data = fp.read().splitlines()[:size]\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQ4inPfZi5wB","colab_type":"code","colab":{}},"source":["def get_attention_for_batch(input_sentences, bs=32):\n","    \"\"\"\n","    This function calculates attention weights of all the heads and\n","    returns it along with the encoded sentence for further processing.\n","    \n","    input sentence: list of strings\n","    bs : batch_size\n","    \"\"\"\n","    \n","    ## Preprocessing for BERT \n","    ids = []\n","    segment_ids = []\n","    input_masks = []\n","    pred_lt = []\n","    ids_for_decoding = []\n","    for sen in input_sentences:\n","        text_tokens = tokenizer.tokenize(sen)\n","        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n","        temp_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        input_mask = [1] * len(temp_ids)\n","        segment_id = [0] * len(temp_ids)\n","        padding = [0] * (max_seq_len - len(temp_ids))\n","        \n","        ids_for_decoding.append(tokenizer.convert_tokens_to_ids(tokens))\n","        temp_ids += padding\n","        input_mask += padding\n","        segment_id += padding\n","        \n","        ids.append(temp_ids)\n","        input_masks.append(input_mask)\n","        segment_ids.append(segment_id)\n","    ## Convert the list of int ids to Torch Tensors\n","    ids = torch.tensor(ids)\n","    segment_ids = torch.tensor(segment_ids)\n","    input_masks = torch.tensor(input_masks)\n","    \n","    steps = len(ids) // bs\n","    \n","    for i in trange(steps+1):\n","        if i == steps:\n","            temp_ids = ids[i * bs : len(ids)]\n","            temp_segment_ids = segment_ids[i * bs: len(ids)]\n","            temp_input_masks = input_masks[i * bs: len(ids)]\n","        else:\n","            temp_ids = ids[i * bs : i * bs + bs]\n","            temp_segment_ids = segment_ids[i * bs: i * bs + bs]\n","            temp_input_masks = input_masks[i * bs: i * bs + bs]\n","        \n","        temp_ids = temp_ids.to(device)\n","        temp_segment_ids = temp_segment_ids.to(device)\n","        temp_input_masks = temp_input_masks.to(device)\n","        \n","        with torch.no_grad():\n","            ##DEBUG\n","            '''\n","            ret = model(temp_ids, temp_segment_ids, temp_input_masks)\n","            attn = ret[-1]\n","            print(ret)\n","            print(attn)\n","            return attn, None\n","            '''\n","            _, _, attn = model(temp_ids, temp_segment_ids, temp_input_masks)\n","            \n","        \n","        # Add all the Attention Weights to CPU memory\n","        # Attention weights for each layer is stored in a dict 'attn_prob'\n","        for k in range(12):\n","            attn[k]['attn_probs'] = attn[k]['attn_probs'].to('cpu')\n","        \n","        '''\n","        attention weights are stored in this way:\n","        att_lt[layer]['attn_probs']['input_sentence']['head']['length_of_sentence']\n","        '''\n","        # Concate Attention weights for all the examples in the list att_lt[layer_no]['attn_probs']\n","        \n","        if i == 0:\n","            att_lt = attn\n","            heads = len(att_lt)\n","        else:\n","            for j in range(heads):\n","                att_lt[j]['attn_probs'] = torch.cat((att_lt[j]['attn_probs'],attn[j]['attn_probs']),0)\n","        \n","    \n","    return att_lt, ids_for_decoding"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGV5KeuCi5wF","colab_type":"code","colab":{}},"source":["def process_sentences(input_sentences, att, decoding_ids, threshold=0.25):\n","    \"\"\"\n","    This function processes each input sentence by removing the top tokens defined threshold value.\n","    Each sentence is processed for each head.\n","    \n","    input_ids: list of strings\n","    decoding_ids: indexed input_sentnces thus len(input_sentences) == len(decoding_ids)\n","    threshold: Percentage of the top indexes to be removed\n","    \"\"\"\n","    # List of None of num_of_layers * num_of_heads to save the results of each head for input_sentences\n","    \n","    lt = [None for x in range(len(att) * len(att[0]['attn_probs'][0]))]\n","    #print(len(lt))\n","    \n","    inx = 0\n","    for i in trange(len(att)): #  For all the layers\n","        for j in range(len(att[i]['attn_probs'][0])): # For all the heads in the ith Layer\n","            processed_sen = [None for q in decoding_ids] # List of len(decoding_ids)\n","            for k in range(len(input_sentences)): # For all the senteces \n","                _, topi = att[i]['attn_probs'][k][j][0].topk(len(decoding_ids[k])) # Get top attended ids\n","                topi = topi.tolist()\n","                topi = topi[:int(len(topi) * threshold)] \n","                ## Decode the sentece after removing the topk indexes\n","                final_indexes = []\n","                count = 0\n","                count1 = 0\n","                tokens = [\"[CLS]\"] + tokenizer.tokenize(input_sentences[k]) + [\"[SEP]\"]\n","                while count < len(decoding_ids[k]):\n","                    if count in topi: # Remove index if present in topk\n","                        while (count + count1 + 1) < len(decoding_ids[k]):\n","                            if \"##\" in tokens[count + count1 + 1]:\n","                                count1 += 1\n","                            else:\n","                                break\n","                        count += count1\n","                        count1 = 0\n","                    else: # Else add to the decoded sentence\n","                        final_indexes.append(decoding_ids[k][count])\n","                    count += 1\n","                tmp = tokenizer.convert_ids_to_tokens(final_indexes) # Convert ids to token\n","                # Convert toknes to sentence\n","                processed_sen[k] = \" \".join(tmp).replace(\" ##\", \"\").replace(\"[CLS]\",\"\").replace(\"[SEP]\",\"\").strip()\n","            lt[inx] = processed_sen # Store sentences for inxth head\n","            inx += 1\n","    \n","    return lt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"77gvhmrCi5wM","colab_type":"code","colab":{}},"source":["def get_block_head(processed_sentence_list, lmbd = 0.1):\n","    \"\"\"\n","    This function calculate classification scores for sentences generated by each head\n","    and sort them from best to worst.\n","    score = min(pred) + lmbd / max(pred) + lmbd, lmbd is smoothing param\n","    pred is list of probability score for each class, for best case pred = [0.5, 0.5] ==> score = 1\n","    \n","    it returns sorted list of (Layer, Head, Score)\n","    \"\"\"\n","    scores = {}\n","    #scores_1 = {}\n","    for i in trange(len(processed_sentence_list)): # sentences by each head\n","        pred = np.array(run_multiple_examples(processed_sentence_list[i]))\n","        scores[i] = np.mean([(min(x[0], x[1])+lmbd)/(max(x[0], x[1])+lmbd) for x in pred])\n","        #scores_1[i] = np.mean([abs(max(x[0],x[1]) - min(x[0],x[1])) for x in pred])\n","    temp = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n","    #temp1 = sorted(scores_1.items(), key=lambda kv: kv[1], reverse=False)\n","    score_lt = [(x // 12, x - (12 * (x // 12)),y) for x,y in temp]\n","    #score1_lt = [(x // 12, x - (12 * (x // 12)),y) for x,y in temp1]\n","    return score_lt  #score1_lt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IUUo8CAi5wS","colab_type":"code","colab":{}},"source":["## TO CUSTOMIZE: input dev file paths (1/0 should be splitting styles.)\n","pos_examples_file = '/content/gdrive/My Drive/humor_style_transfer/reddit_jokes/joke-dataset/style_trans_preprocessing/jokes.dev.1' #sentiment_dev_1\n","neg_examples_file = '/content/gdrive/My Drive/humor_style_transfer/reddit_jokes/joke-dataset/style_trans_preprocessing/jokes.dev.0' #sentiment_dev_0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ziLEUX4Hi5wW","colab_type":"code","colab":{}},"source":["'''\n","100 examples from each class worked good, the bottlenack is the run_multiple_examples() function,\n","with higher memory (either with cpu of gpu) one can reduce the processing time by incresing batch_size.\n","With batch_size of 32 it takes around 24 mins for 100 example on cpu.\n","'''\n","pos_data = read_file(pos_examples_file,100)\n","neg_data = read_file(neg_examples_file,100)\n","data = pos_data + neg_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"So-CXiC1i5wc","colab_type":"code","outputId":"d7b604a1-9b6b-4ad5-89bc-7261d3f33213","executionInfo":{"status":"ok","timestamp":1591164600732,"user_tz":420,"elapsed":410,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(len(pos_data), len(neg_data), len(data))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["100 100 200\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hBqi8B3ji5wi","colab_type":"code","outputId":"5a53f7fc-9c4e-499e-df59-b0c57d27ba46","executionInfo":{"status":"ok","timestamp":1591164607526,"user_tz":420,"elapsed":3365,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["att, decoding_ids = get_attention_for_batch(data)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["100%|██████████| 7/7 [00:02<00:00,  2.46it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"evNTmDiHi5wn","colab_type":"code","outputId":"24dc5bc8-2141-4c3a-f6e1-4e92c4a011a8","executionInfo":{"status":"ok","timestamp":1591164619517,"user_tz":420,"elapsed":8254,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sen_list = process_sentences(data, att, decoding_ids)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["100%|██████████| 12/12 [00:07<00:00,  1.51it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vlZqsOdCi5wt","colab_type":"code","outputId":"487194d3-4a76-4e2c-8351-d9dc3a22358a","executionInfo":{"status":"ok","timestamp":1591164883124,"user_tz":420,"elapsed":255335,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["scores = get_block_head(sen_list)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["100%|██████████| 144/144 [04:14<00:00,  1.77s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eAGwUDOJi5wz","colab_type":"code","outputId":"fd54e277-db1f-4abf-b44f-1da5f9adc8d6","executionInfo":{"status":"ok","timestamp":1591164888657,"user_tz":420,"elapsed":269,"user":{"displayName":"SIDHARTH DHAWAN","photoUrl":"","userId":"04047089348719294795"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["scores\n","#best attn head, reddit: 7,0: 63% accuracy."],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(7, 0, 0.6299214801906022),\n"," (6, 10, 0.6211183162548951),\n"," (6, 2, 0.6183997205361103),\n"," (7, 9, 0.61747919704359),\n"," (7, 7, 0.6167865643790047),\n"," (11, 7, 0.612289415537571),\n"," (8, 4, 0.6098923398537347),\n"," (5, 3, 0.6059533688680397),\n"," (2, 3, 0.6048599540145416),\n"," (7, 11, 0.6041141761825369),\n"," (3, 1, 0.6022005505483667),\n"," (0, 3, 0.6011882482422658),\n"," (6, 3, 0.6011774845645772),\n"," (2, 10, 0.5982901294614612),\n"," (5, 1, 0.5959889324032279),\n"," (2, 2, 0.5952155547473739),\n"," (2, 7, 0.5951336853034459),\n"," (6, 4, 0.5945118816611192),\n"," (3, 10, 0.5944188750228373),\n"," (0, 7, 0.5939774723681766),\n"," (5, 10, 0.5926482457223351),\n"," (6, 0, 0.5923577977027901),\n"," (5, 6, 0.5922820778425112),\n"," (6, 9, 0.5903409990713265),\n"," (2, 11, 0.5897301626567719),\n"," (4, 9, 0.5894368882048094),\n"," (6, 11, 0.5885478652993616),\n"," (8, 1, 0.587257023606776),\n"," (3, 5, 0.5871064145295727),\n"," (6, 1, 0.5869239467938798),\n"," (4, 7, 0.5866291272613833),\n"," (3, 9, 0.5861176354491389),\n"," (11, 6, 0.5858878887742023),\n"," (7, 2, 0.5856338538549112),\n"," (2, 1, 0.5851262278280108),\n"," (5, 9, 0.5848949852152515),\n"," (11, 1, 0.5842045498570583),\n"," (4, 3, 0.5841473438644025),\n"," (1, 5, 0.5835535894322021),\n"," (8, 3, 0.5828190006648587),\n"," (8, 6, 0.5826340406403412),\n"," (0, 10, 0.5826003790329479),\n"," (2, 9, 0.5823288490367653),\n"," (1, 7, 0.581955249820854),\n"," (7, 4, 0.581543493728641),\n"," (1, 9, 0.5814005629768295),\n"," (2, 6, 0.5809819191473797),\n"," (2, 4, 0.5805312616147339),\n"," (4, 0, 0.5803110512061962),\n"," (5, 0, 0.5793676578583734),\n"," (4, 11, 0.5791014928668866),\n"," (10, 9, 0.5790207822228411),\n"," (1, 4, 0.5769634333278595),\n"," (7, 8, 0.5764228243444466),\n"," (3, 6, 0.5758721878394184),\n"," (1, 11, 0.575124285029677),\n"," (4, 10, 0.5745366570384046),\n"," (6, 7, 0.574334200344797),\n"," (11, 9, 0.573888431025943),\n"," (4, 8, 0.5734432252252089),\n"," (2, 0, 0.5723515618527917),\n"," (1, 10, 0.5710966869264604),\n"," (1, 8, 0.5700482582564599),\n"," (10, 4, 0.5699281415601488),\n"," (3, 0, 0.5697132808521773),\n"," (1, 0, 0.5693706286378617),\n"," (1, 1, 0.5692725940245568),\n"," (9, 6, 0.5690227322552284),\n"," (0, 0, 0.5686831016370022),\n"," (0, 11, 0.5682100606167718),\n"," (0, 5, 0.5681219608589125),\n"," (9, 11, 0.5679168408337248),\n"," (3, 7, 0.5677477978388893),\n"," (0, 2, 0.5667790655608197),\n"," (0, 8, 0.5667111006475832),\n"," (4, 5, 0.5664924785223149),\n"," (3, 11, 0.5663131749001598),\n"," (0, 1, 0.5656867269854355),\n"," (5, 5, 0.5655023815495954),\n"," (3, 8, 0.5644762849440723),\n"," (1, 3, 0.5642808179611911),\n"," (4, 4, 0.5633662855565958),\n"," (10, 3, 0.5615401049214955),\n"," (1, 2, 0.5614494131612553),\n"," (3, 3, 0.560255808213226),\n"," (4, 6, 0.5594659065380828),\n"," (10, 5, 0.5591213807778009),\n"," (6, 6, 0.5590377810341738),\n"," (0, 4, 0.5587034691684429),\n"," (8, 10, 0.5580886626777467),\n"," (7, 10, 0.5563953166910665),\n"," (5, 2, 0.5561626046769493),\n"," (11, 0, 0.5560554941946186),\n"," (2, 8, 0.5547764802527319),\n"," (10, 10, 0.5545754401738235),\n"," (11, 10, 0.5544200420240452),\n"," (2, 5, 0.5540665907360366),\n"," (3, 4, 0.5536329057469698),\n"," (8, 5, 0.5519346147928048),\n"," (0, 9, 0.5514577052069989),\n"," (10, 8, 0.5510314605308269),\n"," (10, 7, 0.5509628767148044),\n"," (1, 6, 0.5497035343100132),\n"," (7, 6, 0.5496091871817677),\n"," (9, 9, 0.5486061390809257),\n"," (10, 11, 0.5469082398781793),\n"," (9, 3, 0.5463678039592027),\n"," (10, 2, 0.5458516455327769),\n"," (11, 3, 0.5454256391300872),\n"," (0, 6, 0.5453017297485246),\n"," (9, 8, 0.5443158048485723),\n"," (9, 5, 0.5442522928071372),\n"," (5, 11, 0.5431947534768642),\n"," (9, 4, 0.540686421210237),\n"," (5, 4, 0.5398409088512152),\n"," (3, 2, 0.5393291245566882),\n"," (5, 7, 0.5382401997852109),\n"," (4, 2, 0.5374011517204267),\n"," (8, 2, 0.5351306106513312),\n"," (10, 0, 0.5345194760697942),\n"," (6, 8, 0.5333033930950406),\n"," (10, 1, 0.5310020758783626),\n"," (8, 8, 0.5257693542810169),\n"," (8, 7, 0.5257036483406101),\n"," (8, 11, 0.524701218995938),\n"," (9, 1, 0.5211776458726107),\n"," (11, 2, 0.5202006582915538),\n"," (7, 5, 0.5189618437192139),\n"," (9, 2, 0.5164714923375741),\n"," (7, 3, 0.5155847350888986),\n"," (10, 6, 0.5137525663829762),\n"," (9, 0, 0.5131142643378339),\n"," (7, 1, 0.5123688418312811),\n"," (8, 0, 0.5120954340116118),\n"," (8, 9, 0.5097150696429241),\n"," (11, 11, 0.5096323355126533),\n"," (11, 8, 0.5089935108664347),\n"," (5, 8, 0.5079275140903373),\n"," (6, 5, 0.5045255792732738),\n"," (9, 10, 0.5026465863956766),\n"," (11, 5, 0.4925715697973635),\n"," (9, 7, 0.48927965478958846),\n"," (4, 1, 0.4845558554017362),\n"," (11, 4, 0.4631703288327654)]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"5PK3JR_ci5w2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}